{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 2764486,
          "sourceType": "datasetVersion",
          "datasetId": 1686903
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import imghdr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
        "from collections import Counter\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:29.049371Z",
          "iopub.execute_input": "2025-03-07T07:09:29.049663Z",
          "iopub.status.idle": "2025-03-07T07:09:48.537542Z",
          "shell.execute_reply.started": "2025-03-07T07:09:29.049642Z",
          "shell.execute_reply": "2025-03-07T07:09:48.536605Z"
        },
        "id": "TheUT2-_GBvX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "_lojFOqkkATl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"GPU is set for TensorFlow\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "fj9o68FKkD4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b3la2nYkG2qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dirname, _, filenames in os.walk('/content/drive/Othercomputers/My Laptop/AI-ML/LJMU/Dataset/Brain Tumor/Input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "f2RP4SlhG42l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/Othercomputers/My Laptop/AI-ML/LJMU/Dataset/Brain Tumor/Input'\n",
        "categories = [\"glioma\", \"meningioma\", \"notumor\", \"pituitary\"]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:48.538706Z",
          "iopub.execute_input": "2025-03-07T07:09:48.539166Z",
          "iopub.status.idle": "2025-03-07T07:09:48.543002Z",
          "shell.execute_reply.started": "2025-03-07T07:09:48.539136Z",
          "shell.execute_reply": "2025-03-07T07:09:48.542331Z"
        },
        "id": "jIodGct9GBvZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "for category in categories:\n",
        "    category_path = os.path.join(base_path, category)\n",
        "    for image_name in os.listdir(category_path):\n",
        "        image_path = os.path.join(category_path, image_name)\n",
        "        image_paths.append(image_path)\n",
        "        labels.append(category)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"image_path\": image_paths,\n",
        "    \"label\": labels\n",
        "})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:48.544490Z",
          "iopub.execute_input": "2025-03-07T07:09:48.544719Z",
          "iopub.status.idle": "2025-03-07T07:09:48.593674Z",
          "shell.execute_reply.started": "2025-03-07T07:09:48.544698Z",
          "shell.execute_reply": "2025-03-07T07:09:48.593148Z"
        },
        "id": "bQtTEbtoGBvZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:48.594664Z",
          "iopub.execute_input": "2025-03-07T07:09:48.594920Z",
          "iopub.status.idle": "2025-03-07T07:09:48.634550Z",
          "shell.execute_reply.started": "2025-03-07T07:09:48.594899Z",
          "shell.execute_reply": "2025-03-07T07:09:48.633811Z"
        },
        "id": "jXQ9zLUMGBva"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:48.635348Z",
          "iopub.execute_input": "2025-03-07T07:09:48.635570Z",
          "iopub.status.idle": "2025-03-07T07:09:48.641706Z",
          "shell.execute_reply.started": "2025-03-07T07:09:48.635541Z",
          "shell.execute_reply": "2025-03-07T07:09:48.641067Z"
        },
        "id": "10_dlAVBGBva"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:48.642500Z",
          "iopub.execute_input": "2025-03-07T07:09:48.642824Z",
          "iopub.status.idle": "2025-03-07T07:09:48.657575Z",
          "shell.execute_reply.started": "2025-03-07T07:09:48.642795Z",
          "shell.execute_reply": "2025-03-07T07:09:48.656826Z"
        },
        "id": "D13DEzZqGBva"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Dataset Overview\n",
        "def dataset_overview(df, label_column='label', image_column='image_path'):\n",
        "    print(f\"Total samples: {df.shape}\")\n",
        "    print(f\"Unique classes: {df[label_column].nunique()}\")\n",
        "    print(f\"Class distribution:\\n{df[label_column].value_counts()}\")\n",
        "    print(\"\\nMissing labels:\", df[label_column].isnull().sum())\n",
        "    print(\"Missing image paths:\", df[image_column].isnull().sum())\n",
        "    print(\"Duplicate image paths:\", df.duplicated().sum())\n",
        "    print(\"Info:\", df.info())\n",
        "\n",
        "dataset_overview(df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:48.658268Z",
          "iopub.execute_input": "2025-03-07T07:09:48.658496Z",
          "iopub.status.idle": "2025-03-07T07:09:48.673926Z",
          "shell.execute_reply.started": "2025-03-07T07:09:48.658474Z",
          "shell.execute_reply": "2025-03-07T07:09:48.673167Z"
        },
        "id": "uuCxRYoAGBva"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_invalid_images(df, image_column='image_path', allowed_formats=('jpeg', 'png', 'jpg'),\n",
        "                          save_corrupt_log=False, log_path=\"invalid_images.csv\"):\n",
        "    \"\"\"\n",
        "    Removes rows where image file is missing, unreadable or has invalid format.\n",
        "\n",
        "    Returns:\n",
        "        - cleaned dataframe\n",
        "        - list of invalid file paths\n",
        "    \"\"\"\n",
        "    invalid_images = []\n",
        "\n",
        "    for path in tqdm(df[image_column], desc=\"Validating images\"):\n",
        "        if pd.isna(path) or not os.path.exists(path):\n",
        "            invalid_images.append(path)\n",
        "            continue\n",
        "\n",
        "        # File format check\n",
        "        file_type = imghdr.what(path)\n",
        "        if file_type not in allowed_formats:\n",
        "            invalid_images.append(path)\n",
        "            continue\n",
        "\n",
        "        # Readability check\n",
        "        try:\n",
        "            img = cv2.imread(path)\n",
        "            if img is None:\n",
        "                invalid_images.append(path)\n",
        "        except:\n",
        "            invalid_images.append(path)\n",
        "\n",
        "    print(f\"\\nTotal invalid images detected: {len(invalid_images)}\")\n",
        "\n",
        "    df_clean = df[~df[image_column].isin(invalid_images)].reset_index(drop=True)\n",
        "    print(f\"Remaining clean samples: {len(df_clean)}\")\n",
        "\n",
        "    if save_corrupt_log and invalid_images:\n",
        "        pd.DataFrame(invalid_images, columns=['invalid_path']).to_csv(log_path, index=False)\n",
        "        print(f\"Invalid image paths saved to: {log_path}\")\n",
        "\n",
        "    return df_clean, invalid_images\n",
        "\n",
        "df_clean, invalid_files = clean_invalid_images(df, image_column='image_path', allowed_formats=('jpeg', 'png', 'jpg'))\n",
        "\n",
        "df = df_clean"
      ],
      "metadata": {
        "id": "dMDWQAA1_7nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis\n"
      ],
      "metadata": {
        "id": "Vzix8Ugj7jfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Dimension Analysis\n",
        "def image_dimensions_analysis(df, image_column='image_path'):\n",
        "    widths, heights = [], []\n",
        "    for path in tqdm(df[image_column], desc=\"Reading image dimensions\"):\n",
        "        try:\n",
        "            img = Image.open(path)\n",
        "            w, h = img.size\n",
        "            widths.append(w)\n",
        "            heights.append(h)\n",
        "        except:\n",
        "            continue\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(widths, kde=True, color=\"skyblue\", label=\"Width\")\n",
        "    sns.histplot(heights, kde=True, color=\"salmon\", label=\"Height\")\n",
        "    plt.title(\"Image Dimension Distribution\")\n",
        "    plt.xlabel(\"Pixels\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f\"Width: min={min(widths)}, max={max(widths)}, mean={np.mean(widths):.2f}\")\n",
        "    print(f\"Height: min={min(heights)}, max={max(heights)}, mean={np.mean(heights):.2f}\")\n",
        "\n",
        "\n",
        "image_dimensions_analysis(df)"
      ],
      "metadata": {
        "id": "x0_yK6AwJ2KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aspect Ratio Distribution\n",
        "def aspect_ratio_distribution(df, image_column='image_path'):\n",
        "    aspect_ratios = []\n",
        "    for path in tqdm(df[image_column], desc=\"Computing aspect ratios\"):\n",
        "        try:\n",
        "            img = Image.open(path)\n",
        "            w, h = img.size\n",
        "            aspect_ratios.append(w/h)\n",
        "        except:\n",
        "            continue\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(aspect_ratios, kde=True, color=\"purple\")\n",
        "    plt.title(\"Aspect Ratio Distribution\")\n",
        "    plt.xlabel(\"Width / Height\")\n",
        "    plt.show()\n",
        "\n",
        "aspect_ratio_distribution(df)"
      ],
      "metadata": {
        "id": "E_WANpkJKB9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per Channel Mean/Std (for normalization)\n",
        "def image_channel_statistics(df, image_column='image_path', sample_size=1000):\n",
        "    means, stds = [], []\n",
        "    sampled_paths = df[image_column].sample(min(len(df), sample_size), random_state=42)\n",
        "\n",
        "    for path in tqdm(sampled_paths, desc=\"Computing channel stats\"):\n",
        "        try:\n",
        "            img = cv2.imread(path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
        "            means.append(np.mean(img, axis=(0, 1)))\n",
        "            stds.append(np.std(img, axis=(0, 1)))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    mean = np.mean(means, axis=0)\n",
        "    std = np.mean(stds, axis=0)\n",
        "    print(f\"Channel-wise Mean (R, G, B): {mean}\")\n",
        "    print(f\"Channel-wise Std (R, G, B): {std}\")\n",
        "    return mean, std\n",
        "\n",
        "# Channel statistics\n",
        "mean, std = image_channel_statistics(df)"
      ],
      "metadata": {
        "id": "iTGRCDkLACVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_category_samples(df, categories, num_images=5, figsize=(15, 12)):\n",
        "    \"\"\"\n",
        "    Display sample images for each category.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing 'label' and 'image_path' columns.\n",
        "    - categories: list of category labels.\n",
        "    - num_images: number of images to display per category.\n",
        "    - figsize: size of the entire figure.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    for i, category in enumerate(categories):\n",
        "        category_images = df[df['label'] == category]['image_path'].iloc[:num_images]\n",
        "\n",
        "        for j, img_path in enumerate(category_images):\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Warning: Image not found at {img_path}\")\n",
        "                continue  # Skip if image not loaded\n",
        "\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            ax = plt.subplot(len(categories), num_images, i * num_images + j + 1)\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "\n",
        "            # Only add title for first image of each category\n",
        "            if j == 0:\n",
        "                ax.set_title(f\"Class: {category}\", fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ctCWfOZP8yoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_category_samples(df, categories, num_images=5)"
      ],
      "metadata": {
        "id": "z_ElJica9A6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_distribution(df, label_column=\"label\", figsize=(16, 6), palette=\"viridis\", font_size=14, title_size=16):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Create subplots for side-by-side display\n",
        "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "    # Countplot\n",
        "    ax1 = sns.countplot(data=df, x=label_column, palette=palette, ax=axes[0])\n",
        "    ax1.set_title(f\"Distribution of {label_column.capitalize()} - Count Plot\", fontsize=title_size)\n",
        "    ax1.set_xlabel(label_column.capitalize(), fontsize=font_size)\n",
        "    ax1.set_ylabel(\"Count\", fontsize=font_size)\n",
        "    ax1.tick_params(axis='both', labelsize=font_size)\n",
        "\n",
        "    for p in ax1.patches:\n",
        "        ax1.annotate(f'{int(p.get_height())}',\n",
        "                     (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                     ha='center', va='center', fontsize=font_size, color='black', xytext=(0, 5),\n",
        "                     textcoords='offset points')\n",
        "\n",
        "    # Pie Chart\n",
        "    label_counts = df[label_column].value_counts()\n",
        "    wedges, texts, autotexts = axes[1].pie(\n",
        "        label_counts,\n",
        "        labels=label_counts.index,\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=140,\n",
        "        colors=sns.color_palette(palette, n_colors=len(label_counts)),\n",
        "        textprops={'fontsize': font_size}\n",
        "    )\n",
        "    axes[1].set_title(f\"Distribution of {label_column.capitalize()} - Pie Chart\", fontsize=title_size)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "label_distribution(df, font_size=14, title_size=18)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:48.787571Z",
          "iopub.execute_input": "2025-03-07T07:09:48.787817Z",
          "iopub.status.idle": "2025-03-07T07:09:50.237441Z",
          "shell.execute_reply.started": "2025-03-07T07:09:48.787796Z",
          "shell.execute_reply": "2025-03-07T07:09:50.236623Z"
        },
        "id": "AaHNpYpEGBvb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label Encoding"
      ],
      "metadata": {
        "id": "9o0Ra8V08Lnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:54.000600Z",
          "iopub.execute_input": "2025-03-07T07:09:54.001032Z",
          "iopub.status.idle": "2025-03-07T07:09:54.114535Z",
          "shell.execute_reply.started": "2025-03-07T07:09:54.000976Z",
          "shell.execute_reply": "2025-03-07T07:09:54.113918Z"
        },
        "id": "QWj-MYS6GBvb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['image_path', 'label_encoded']]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:54.115273Z",
          "iopub.execute_input": "2025-03-07T07:09:54.115551Z",
          "iopub.status.idle": "2025-03-07T07:09:54.123260Z",
          "shell.execute_reply.started": "2025-03-07T07:09:54.115529Z",
          "shell.execute_reply": "2025-03-07T07:09:54.122422Z"
        },
        "id": "EhFpjPGUGBvb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resampling"
      ],
      "metadata": {
        "id": "V72ZAK_4g55_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(df[['image_path']], df['label_encoded'])\n",
        "\n",
        "df_resampled = pd.DataFrame(X_resampled, columns=['image_path'])\n",
        "df_resampled['label_encoded'] = y_resampled"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:54.123966Z",
          "iopub.execute_input": "2025-03-07T07:09:54.124255Z",
          "iopub.status.idle": "2025-03-07T07:09:55.330777Z",
          "shell.execute_reply.started": "2025-03-07T07:09:54.124226Z",
          "shell.execute_reply": "2025-03-07T07:09:55.330160Z"
        },
        "id": "liKPHIJRGBvb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClass distribution after oversampling:\")\n",
        "print(df_resampled['label_encoded'].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:55.331470Z",
          "iopub.execute_input": "2025-03-07T07:09:55.331742Z",
          "iopub.status.idle": "2025-03-07T07:09:55.339744Z",
          "shell.execute_reply.started": "2025-03-07T07:09:55.331722Z",
          "shell.execute_reply": "2025-03-07T07:09:55.338772Z"
        },
        "id": "vyzrOfKzGBvb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_resampled"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:55.340475Z",
          "iopub.execute_input": "2025-03-07T07:09:55.340686Z",
          "iopub.status.idle": "2025-03-07T07:09:55.362101Z",
          "shell.execute_reply.started": "2025-03-07T07:09:55.340666Z",
          "shell.execute_reply": "2025-03-07T07:09:55.361339Z"
        },
        "id": "gdhOGBxwGBvb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_resampled['label_encoded'] = df_resampled['label_encoded'].astype(str)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:09:55.362832Z",
          "iopub.execute_input": "2025-03-07T07:09:55.363132Z",
          "iopub.status.idle": "2025-03-07T07:09:55.383396Z",
          "shell.execute_reply.started": "2025-03-07T07:09:55.363110Z",
          "shell.execute_reply": "2025-03-07T07:09:55.382811Z"
        },
        "id": "i4TuOA1VGBvc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "label_distribution(df_resampled, label_column= \"label_encoded\",font_size=14, title_size=18)"
      ],
      "metadata": {
        "id": "Tig6HVDRnnLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Test Validation Split / Image genarator"
      ],
      "metadata": {
        "id": "BCIkjcvB8Azp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_new, temp_df_new = train_test_split(\n",
        "    df_resampled,\n",
        "    train_size=0.8,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=df_resampled['label_encoded']\n",
        ")\n",
        "\n",
        "valid_df_new, test_df_new = train_test_split(\n",
        "    temp_df_new,\n",
        "    test_size=0.5,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=temp_df_new['label_encoded']\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:10:12.350914Z",
          "iopub.execute_input": "2025-03-07T07:10:12.351439Z",
          "iopub.status.idle": "2025-03-07T07:10:12.381161Z",
          "shell.execute_reply.started": "2025-03-07T07:10:12.351414Z",
          "shell.execute_reply": "2025-03-07T07:10:12.380474Z"
        },
        "id": "qnfsHW9QGBvc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "\n",
        "tr_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,           # Slightly increased for better variation\n",
        "    width_shift_range=0.05,      # This mimics slight off-center placements that happen in real MRI scans.\n",
        "    height_shift_range=0.05,     # This mimics slight off-center placements that happen in real MRI scans.\n",
        "    zoom_range=0.1,\n",
        "    channel_shift_range = 0.05,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.9, 1.1], # Simulates brightness differences in scans\n",
        "    shear_range=5,               # Mild shearing to simulate spatial warping\n",
        "    fill_mode='nearest'          # Fills empty pixels after transformations\n",
        ")\n",
        "\n",
        "ts_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen_new = tr_gen.flow_from_dataframe(\n",
        "    train_df_new,\n",
        "    x_col='image_path',\n",
        "    y_col='label_encoded',\n",
        "    target_size=img_size,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_gen_new = ts_gen.flow_from_dataframe(\n",
        "    valid_df_new,\n",
        "    x_col='image_path',\n",
        "    y_col='label_encoded',\n",
        "    target_size=img_size,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb',\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_gen_new = ts_gen.flow_from_dataframe(\n",
        "    test_df_new,\n",
        "    x_col='image_path',\n",
        "    y_col='label_encoded',\n",
        "    target_size=img_size,\n",
        "    class_mode='sparse',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:10:12.381889Z",
          "iopub.execute_input": "2025-03-07T07:10:12.382181Z",
          "iopub.status.idle": "2025-03-07T07:10:21.246728Z",
          "shell.execute_reply.started": "2025-03-07T07:10:12.382143Z",
          "shell.execute_reply": "2025-03-07T07:10:21.245980Z"
        },
        "id": "6TY0TcDgGBvc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Functions"
      ],
      "metadata": {
        "id": "gEdK8NBr72sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_callbacks(model_name, monitor_metric='val_loss', patience=5, save_dir='./'):\n",
        "    \"\"\"\n",
        "    Generate EarlyStopping and ModelCheckpoint callbacks dynamically.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): Name of the model (will be used for the filename)\n",
        "    - monitor_metric (str): Metric to monitor (default: 'val_accuracy')\n",
        "    - patience (int): Number of epochs to wait before early stopping\n",
        "    - save_dir (str): Directory where model should be saved (default: current directory)\n",
        "\n",
        "    Returns:\n",
        "    - List of callbacks [early_stopping, model_checkpoint]\n",
        "    \"\"\"\n",
        "    # Build full model save path\n",
        "    model_path = f\"{save_dir}/best_{model_name}.keras\"\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=monitor_metric,\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Model checkpoint callback\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        model_path,\n",
        "        monitor=monitor_metric,\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor=monitor_metric,\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1,\n",
        "        mode='min'  # IMPORTANT for accuracy!\n",
        "    )\n",
        "\n",
        "    return [early_stopping, model_checkpoint, reduce_lr]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:10:22.473214Z",
          "iopub.execute_input": "2025-03-07T07:10:22.473457Z",
          "iopub.status.idle": "2025-03-07T07:10:22.486368Z",
          "shell.execute_reply.started": "2025-03-07T07:10:22.473431Z",
          "shell.execute_reply": "2025-03-07T07:10:22.485764Z"
        },
        "id": "s0t_WV8fGBvc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, metrics=['accuracy', 'loss']):\n",
        "    \"\"\"\n",
        "    Plots training history for given metrics side by side.\n",
        "\n",
        "    Parameters:\n",
        "    - history: History object returned by model.fit()\n",
        "    - metrics: List of metrics to plot (default: ['accuracy', 'loss'])\n",
        "    \"\"\"\n",
        "    n_metrics = len(metrics)\n",
        "    fig, axes = plt.subplots(1, n_metrics, figsize=(7 * n_metrics, 5))  # side by side\n",
        "\n",
        "    if n_metrics == 1:\n",
        "        axes = [axes]  # make iterable if only one metric\n",
        "\n",
        "    for ax, metric in zip(axes, metrics):\n",
        "        if metric in history.history:\n",
        "            ax.plot(history.history[metric], label='Train')\n",
        "            ax.plot(history.history['val_' + metric], label='Validation')\n",
        "            ax.set_title(f'Model {metric.capitalize()}')\n",
        "            ax.set_ylabel(metric.capitalize())\n",
        "            ax.set_xlabel('Epoch')\n",
        "            ax.legend(loc='upper left')\n",
        "            ax.grid(True)\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, f\"⚠️ Metric '{metric}' not found\",\n",
        "                    ha='center', va='center', fontsize=12)\n",
        "            ax.set_axis_off()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ZITonKj_1QS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_performance(true_labels, predicted_labels, class_names, figsize=(10, 8), cmap='Blues'):\n",
        "    \"\"\"\n",
        "    Prints classification report and displays confusion matrix heatmap.\n",
        "\n",
        "    Parameters:\n",
        "    - true_labels: Ground truth labels\n",
        "    - predicted_labels: Predicted labels from the model\n",
        "    - class_names: List of class names (usually from test_gen.class_indices.keys())\n",
        "    - figsize: Tuple for the size of the heatmap figure\n",
        "    - cmap: Color map for heatmap\n",
        "    \"\"\"\n",
        "    # Classification report\n",
        "    report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "\n",
        "    # Confusion matrix\n",
        "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=cmap,\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lhtj6GEK5AEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global results collector\n",
        "model_results = []\n",
        "\n",
        "def record_model_results(model_name, history, test_generator, model):\n",
        "    \"\"\"\n",
        "    Records train/val/test accuracy, precision, recall, and F1 score.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): Name of the model (ViT/Swin/MaxViT)\n",
        "    - history: History object from model.fit()\n",
        "    - test_generator: Generator or dataset used for testing\n",
        "    - model: Trained Keras model\n",
        "    \"\"\"\n",
        "    train_acc = history.history['accuracy'][-1]\n",
        "    val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "    # Get test labels and predictions\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in test_generator:\n",
        "        x_batch, y_batch = batch\n",
        "        preds = model.predict(x_batch, verbose=0)\n",
        "        y_true.extend(y_batch)\n",
        "        y_pred.extend(np.argmax(preds, axis=1))\n",
        "        if len(y_true) >= test_generator.samples:\n",
        "            break\n",
        "\n",
        "    y_true = np.array(y_true)[:test_generator.samples]\n",
        "    y_pred = np.array(y_pred)[:test_generator.samples]\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
        "\n",
        "    model_results.append({\n",
        "        'Model': model_name,\n",
        "        'Train Acc': round(train_acc * 100, 2),\n",
        "        'Val Acc': round(val_acc * 100, 2),\n",
        "        'Test Acc': round(test_acc * 100, 2),\n",
        "        'Precision': round(precision * 100, 2),\n",
        "        'Recall': round(recall * 100, 2),\n",
        "        'F1 Score': round(f1 * 100, 2)\n",
        "    })\n",
        "\n",
        "def display_results_table():\n",
        "    df = pd.DataFrame(model_results)\n",
        "    display(df)\n"
      ],
      "metadata": {
        "id": "agJYWk_MFKgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer"
      ],
      "metadata": {
        "id": "V1EJkWO6GBvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. DropPath Layer\n",
        "class DropPath(layers.Layer):\n",
        "    def __init__(self, drop_prob=0.0):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        if (not training) or (self.drop_prob == 0.0):\n",
        "            return x\n",
        "        keep_prob = 1.0 - self.drop_prob\n",
        "        shape = (tf.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n",
        "        random_tensor = keep_prob + tf.random.uniform(shape, dtype=x.dtype)\n",
        "        binary_tensor = tf.floor(random_tensor)\n",
        "        return tf.math.divide(x, keep_prob) * binary_tensor\n",
        "\n",
        "### 2. Patch Embedding Layer\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, patch_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = layers.Conv2D(embed_dim, patch_size, strides=patch_size, padding='valid')\n",
        "\n",
        "    def call(self, images):\n",
        "        patches = self.proj(images)\n",
        "        return tf.reshape(patches, (tf.shape(images)[0], -1, patches.shape[-1]))\n",
        "\n",
        "### 3. Multi-Head Self Attention with Tracking\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, num_heads, embed_dim):\n",
        "        super().__init__()\n",
        "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attn_weights = None\n",
        "\n",
        "    def call(self, x):\n",
        "        output, weights = self.attn(x, x, return_attention_scores=True)\n",
        "        self.attn_weights = weights\n",
        "        return output\n",
        "\n",
        "### 4. Transformer Block with DropPath\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate, drop_path_rate):\n",
        "        super().__init__()\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.attn = MultiHeadSelfAttention(num_heads, embed_dim)\n",
        "        self.drop_path1 = DropPath(drop_path_rate)\n",
        "\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            layers.Dense(mlp_dim, activation='gelu'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(embed_dim),\n",
        "            layers.Dropout(dropout_rate),\n",
        "        ])\n",
        "        self.drop_path2 = DropPath(drop_path_rate)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attn(x)\n",
        "        x = shortcut + self.drop_path1(x, training=training)\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x, training=training)\n",
        "        x = shortcut + self.drop_path2(x, training=training)\n",
        "        return x\n",
        "\n",
        "### 5. Vision Transformer Model\n",
        "class VisionTransformer(tf.keras.Model):\n",
        "    def __init__(self, image_size, patch_size, embed_dim, num_heads, num_blocks, mlp_dim,\n",
        "                 num_classes, dropout_rate=0.1, max_drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n",
        "\n",
        "        height, width, _ = image_size\n",
        "        num_patches = (height // patch_size) * (width // patch_size)\n",
        "        self.cls_token = self.add_weight(name=\"cls_token\", shape=(1, 1, embed_dim), initializer=\"random_normal\", trainable=True)\n",
        "        self.pos_embed = self.add_weight(name=\"pos_embed\", shape=(1, num_patches + 1, embed_dim), initializer=\"random_normal\", trainable=True)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "        drop_path_rates = np.linspace(0.0, max_drop_path_rate, num_blocks)\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout_rate, drop_path_rate)\n",
        "            for drop_path_rate in drop_path_rates\n",
        "        ]\n",
        "\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.head = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, images, training=False):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        x = self.patch_embed(images)\n",
        "\n",
        "        cls_tokens = tf.broadcast_to(self.cls_token, [batch_size, 1, x.shape[-1]])\n",
        "        x = tf.concat([cls_tokens, x], axis=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "    def get_attention_weights(self):\n",
        "        return [block.attn.attn_weights for block in self.transformer_blocks]\n",
        "\n",
        "image_size = (224, 224, 3)\n",
        "patch_size = 16\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "num_blocks = 6\n",
        "mlp_dim = 256\n",
        "num_classes = 4\n",
        "dropout_rate = 0.1\n",
        "max_drop_path_rate = 0.1\n",
        "\n",
        "# Changed to a fixed learning rate to work with ReduceLROnPlateau\n",
        "learning_rate = 2e-4\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "vit_model = VisionTransformer(image_size=image_size,\n",
        "                              patch_size=patch_size,\n",
        "                              embed_dim=embed_dim,\n",
        "                              num_heads=num_heads,\n",
        "                              num_blocks=num_blocks,\n",
        "                              mlp_dim=mlp_dim,\n",
        "                              num_classes=num_classes,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              max_drop_path_rate=max_drop_path_rate)\n",
        "\n",
        "vit_model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:15:39.955786Z",
          "iopub.execute_input": "2025-03-07T07:15:39.956085Z",
          "iopub.status.idle": "2025-03-07T07:15:40.039214Z",
          "shell.execute_reply.started": "2025-03-07T07:15:39.956059Z",
          "shell.execute_reply": "2025-03-07T07:15:40.038493Z"
        },
        "id": "qfojcCDSGBvd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 80\n",
        "callbacks = get_callbacks(model_name='vit_model')\n",
        "vit_history = vit_model.fit(train_gen_new, epochs=epochs, batch_size = 32, validation_data=valid_gen_new, callbacks=callbacks)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:15:40.040022Z",
          "iopub.execute_input": "2025-03-07T07:15:40.040300Z",
          "iopub.status.idle": "2025-03-07T07:24:11.080517Z",
          "shell.execute_reply.started": "2025-03-07T07:15:40.040273Z",
          "shell.execute_reply": "2025-03-07T07:24:11.079621Z"
        },
        "id": "v5jXgtOUGBvd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(vit_history)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:24:11.081860Z",
          "iopub.execute_input": "2025-03-07T07:24:11.082264Z",
          "iopub.status.idle": "2025-03-07T07:24:11.484730Z",
          "shell.execute_reply.started": "2025-03-07T07:24:11.082222Z",
          "shell.execute_reply": "2025-03-07T07:24:11.484029Z"
        },
        "id": "BoEQQkLHGBve"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = test_gen_new.classes\n",
        "predictions = vit_model.predict(test_gen_new)\n",
        "predicted_classes = np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:24:11.485613Z",
          "iopub.execute_input": "2025-03-07T07:24:11.485931Z",
          "iopub.status.idle": "2025-03-07T07:24:23.998082Z",
          "shell.execute_reply.started": "2025-03-07T07:24:11.485897Z",
          "shell.execute_reply": "2025-03-07T07:24:23.997105Z"
        },
        "id": "D4WC9a_1GBve"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_performance(test_labels, predicted_classes, list(test_gen_new.class_indices.keys()), figsize=(10, 8), cmap='Blues')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:24:23.999114Z",
          "iopub.execute_input": "2025-03-07T07:24:23.999462Z",
          "iopub.status.idle": "2025-03-07T07:24:24.012721Z",
          "shell.execute_reply.started": "2025-03-07T07:24:23.999416Z",
          "shell.execute_reply": "2025-03-07T07:24:24.012089Z"
        },
        "id": "4ukBEgHnGBve"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "record_model_results(\"ViT\", vit_history, test_gen_new, vit_model)\n",
        "display_results_table()"
      ],
      "metadata": {
        "id": "ede-4hM7FuF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin Transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "H58P_CwZE_LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowPartition(layers.Layer):\n",
        "    def __init__(self, window_size):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def call(self, x):\n",
        "        B, H, W, C = tf.shape(x)[0], x.shape[1], x.shape[2], x.shape[3]\n",
        "        x = tf.reshape(x, [B, H // self.window_size, self.window_size,\n",
        "                           W // self.window_size, self.window_size, C])\n",
        "        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n",
        "        windows = tf.reshape(x, [-1, self.window_size * self.window_size, C])\n",
        "        return windows\n",
        "\n",
        "class WindowReverse(layers.Layer):\n",
        "    def __init__(self, window_size, H, W):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.H = H\n",
        "        self.W = W\n",
        "\n",
        "    def call(self, windows, B):\n",
        "        x = tf.reshape(windows, [B, self.H // self.window_size, self.W // self.window_size,\n",
        "                                 self.window_size, self.window_size, -1])\n",
        "        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])\n",
        "        x = tf.reshape(x, [B, self.H, self.W, -1])\n",
        "        return x\n",
        "\n",
        "class WindowAttention(layers.Layer):\n",
        "    def __init__(self, dim, num_heads, window_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.qkv = layers.Dense(dim * 3, use_bias=False)\n",
        "        self.proj = layers.Dense(dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        B_, N, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = tf.reshape(qkv, (B_, N, 3, self.num_heads, C // self.num_heads))\n",
        "        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "        attn = tf.nn.softmax(attn)\n",
        "        x = tf.matmul(attn, v)\n",
        "        x = tf.transpose(x, [0, 2, 1, 3])\n",
        "        x = tf.reshape(x, (B_, N, C))\n",
        "        return self.proj(x)\n",
        "\n",
        "class SwinTransformerBlock(layers.Layer):\n",
        "    def __init__(self, dim, num_heads, window_size, shift_size=0):\n",
        "        super().__init__()\n",
        "        self.norm1 = layers.LayerNormalization()\n",
        "        self.attn = WindowAttention(dim, num_heads, window_size)\n",
        "        self.norm2 = layers.LayerNormalization()\n",
        "        self.mlp = [\n",
        "            layers.Dense(dim * 4, activation='gelu'),\n",
        "            layers.Dense(dim)\n",
        "        ]\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "\n",
        "    def call(self, x, H, W):\n",
        "        B, L, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
        "        x = tf.reshape(x, (B, H, W, C))\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2])\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        x_windows = WindowPartition(self.window_size)(shifted_x)\n",
        "        attn_windows = self.attn(x_windows)\n",
        "        shifted_x = WindowReverse(self.window_size, H, W)(attn_windows, B)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2])\n",
        "        else:\n",
        "            x = shifted_x\n",
        "\n",
        "        x = tf.reshape(x, (B, H * W, C))\n",
        "        residual = x\n",
        "        x = self.norm2(x + self.attn(self.norm1(x))) # Fixed: Passing only tensor to self.attn\n",
        "        for mlp_layer in self.mlp: # Fixed: Iterating through mlp layers directly\n",
        "          x = mlp_layer(x)\n",
        "        x = residual + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchMerging(layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.reduction = layers.Dense(input_dim * 2, use_bias=False)\n",
        "\n",
        "    def call(self, x, H, W):\n",
        "        B, L, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
        "        x = tf.reshape(x, (B, H, W, C))\n",
        "        x0 = x[:, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, :]\n",
        "        x3 = x[:, 1::2, 1::2, :]\n",
        "        x = tf.concat([x0, x1, x2, x3], axis=-1)\n",
        "        x = tf.reshape(x, (B, -1, 4 * C))\n",
        "        return self.reduction(x)\n",
        "\n",
        "\n",
        "class SwinTransformer(layers.Layer):\n",
        "    def __init__(self, input_shape, patch_size=4, embed_dim=96, depths=[2,2], num_heads=[3,6], window_size=7):\n",
        "        super().__init__()\n",
        "        self.patch_embed = layers.Conv2D(embed_dim, patch_size, strides=patch_size, padding='same')\n",
        "        self.pos_drop = layers.Dropout(0.1)\n",
        "        self.layers = []\n",
        "        self.H, self.W = input_shape[0] // patch_size, input_shape[1] // patch_size\n",
        "\n",
        "\n",
        "        for i_layer in range(len(depths)):\n",
        "            for i_block in range(depths[i_layer]):\n",
        "                shift_size = 0 if (i_block % 2 == 0) else window_size // 2\n",
        "                self.layers.append(\n",
        "                    SwinTransformerBlock(embed_dim * (2 ** i_layer), num_heads[i_layer], window_size, shift_size)\n",
        "                )\n",
        "            if i_layer < len(depths) - 1:\n",
        "                self.layers.append(PatchMerging(embed_dim * (2 ** i_layer)))\n",
        "\n",
        "\n",
        "        self.norm = layers.LayerNormalization()\n",
        "        self.pool = layers.GlobalAveragePooling1D()\n",
        "        self.fc = layers.Dense(4, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        H, W = x.shape[1], x.shape[2]\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1, x.shape[-1]))\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, SwinTransformerBlock):\n",
        "                x = layer(x, H=H, W=W)\n",
        "            elif isinstance(layer, PatchMerging):\n",
        "                x = layer(x, H=H, W=W)\n",
        "                H, W = H // 2, W // 2\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.pool(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "input_shape = (224, 224, 3)\n",
        "swin_model = Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    SwinTransformer(input_shape=input_shape)\n",
        "])"
      ],
      "metadata": {
        "id": "dKuJ6cW8Eubv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed to a fixed learning rate to work with ReduceLROnPlateau\n",
        "learning_rate = 2e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "swin_model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks = get_callbacks(model_name='swin_model')\n",
        "\n",
        "swin_history = swin_model.fit(\n",
        "    train_gen_new,\n",
        "    validation_data=valid_gen_new,\n",
        "    epochs=80,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T07:24:24.026267Z",
          "iopub.execute_input": "2025-03-07T07:24:24.026598Z",
          "iopub.status.idle": "2025-03-07T07:24:24.296774Z",
          "shell.execute_reply.started": "2025-03-07T07:24:24.026569Z",
          "shell.execute_reply": "2025-03-07T07:24:24.295811Z"
        },
        "id": "ZjpC0IZSGBve"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(swin_history)"
      ],
      "metadata": {
        "id": "MPm2r7J_FVsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = test_gen_new.classes\n",
        "predictions = swin_model.predict(test_gen_new)\n",
        "predicted_classes = np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "OXPvXWSqGrtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_performance(test_labels, predicted_classes, list(test_gen_new.class_indices.keys()), figsize=(10, 8), cmap='Blues')"
      ],
      "metadata": {
        "id": "gWwm_1_TGq1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_model_results(\"Swin\", swin_history, test_gen_new, swin_model)\n",
        "display_results_table()"
      ],
      "metadata": {
        "id": "NheU2096HKSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MaxVit Transformer"
      ],
      "metadata": {
        "id": "o4gnIzdF3nRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Hyperparameters ===\n",
        "batch_size = 32\n",
        "img_size = (224, 224, 3)\n",
        "patch_size = 16\n",
        "embed_dim = 64\n",
        "num_heads = 4\n",
        "window_size = 7\n",
        "num_blocks = 2\n",
        "mlp_dim = 128\n",
        "num_classes = 4\n",
        "dropout_rate = 0.1\n",
        "weight_decay = 1e-4\n",
        "\n",
        "initial_learning_rate = 1e-4\n",
        "# === MBConv Block ===\n",
        "class MBConv(layers.Layer):\n",
        "    def __init__(self, embed_dim, expansion_factor=4):\n",
        "        super().__init__()\n",
        "        self.expand = layers.Conv2D(embed_dim * expansion_factor, 1, padding='same', activation='gelu')\n",
        "        self.depthwise = layers.DepthwiseConv2D(3, padding='same', activation='gelu')\n",
        "        self.project = layers.Conv2D(embed_dim, 1, padding='same')\n",
        "        self.norm = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        residual = x\n",
        "        x = self.expand(x)\n",
        "        x = self.depthwise(x)\n",
        "        x = self.project(x)\n",
        "        return self.norm(x + residual)\n",
        "\n",
        "# === Local and Grid Attention (Simplified) ===\n",
        "class WindowAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, window_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "\n",
        "    def call(self, x):\n",
        "        B = tf.shape(x)[0]\n",
        "        H, W, C = x.shape[1], x.shape[2], x.shape[3]\n",
        "        x_reshaped = tf.reshape(x, (B, H // self.window_size, self.window_size, W // self.window_size, self.window_size, C))\n",
        "        x_reshaped = tf.transpose(x_reshaped, [0, 1, 3, 2, 4, 5])\n",
        "        x_windows = tf.reshape(x_reshaped, (-1, self.window_size * self.window_size, C))\n",
        "        attn_windows = self.attn(x_windows, x_windows)\n",
        "        attn_windows = tf.reshape(attn_windows, (B, H // self.window_size, W // self.window_size, self.window_size, self.window_size, C))\n",
        "        attn_windows = tf.transpose(attn_windows, [0, 1, 3, 2, 4, 5])\n",
        "        return tf.reshape(attn_windows, (B, H, W, C))\n",
        "\n",
        "class GridAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "\n",
        "    def call(self, x):\n",
        "        B, H, W, C = tf.shape(x)[0], x.shape[1], x.shape[2], x.shape[3]\n",
        "        x = tf.transpose(x, [0, 2, 1, 3])\n",
        "        x = tf.reshape(x, (B * W, H, C))\n",
        "        x = self.attn(x, x)\n",
        "        x = tf.reshape(x, (B, W, H, C))\n",
        "        return tf.transpose(x, [0, 2, 1, 3])\n",
        "\n",
        "# === MaxViT Block ===\n",
        "class MaxViTBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, window_size):\n",
        "        super().__init__()\n",
        "        self.mbconv = MBConv(embed_dim)\n",
        "        self.window_attn = WindowAttention(embed_dim, num_heads, window_size)\n",
        "        self.grid_attn = GridAttention(embed_dim, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Conv2D(embed_dim * 2, 1, activation='gelu'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Conv2D(embed_dim, 1),\n",
        "            layers.Dropout(dropout_rate),\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.mbconv(x)\n",
        "        win_out = self.window_attn(x)\n",
        "        x = x + win_out\n",
        "        x = x + self.grid_attn(x)\n",
        "        x = x + self.ffn(x)\n",
        "        return x\n",
        "\n",
        "# === Patch Embedding Layer ===\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, patch_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = layers.Conv2D(embed_dim, patch_size, strides=patch_size, padding='valid')\n",
        "\n",
        "    def call(self, images):\n",
        "        return self.proj(images)\n",
        "\n",
        "# === MaxViT Model ===\n",
        "class MaxViT(tf.keras.Model):\n",
        "    def __init__(self, image_size, patch_size, embed_dim, num_heads, num_blocks, window_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n",
        "        self.pos_drop = layers.Dropout(dropout_rate)\n",
        "        self.blocks = [MaxViTBlock(embed_dim, num_heads, window_size) for _ in range(num_blocks)]\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.pool = layers.GlobalAveragePooling2D()\n",
        "        self.head = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.pos_drop(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.pool(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# === Instantiate and Compile ===\n",
        "maxVit_model = MaxViT(\n",
        "    image_size=img_size,\n",
        "    patch_size=patch_size,\n",
        "    embed_dim=embed_dim,\n",
        "    num_heads=num_heads,\n",
        "    num_blocks=num_blocks,\n",
        "    window_size=window_size,\n",
        "    num_classes=num_classes\n",
        ")"
      ],
      "metadata": {
        "id": "vMIC5ymb3m97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changed to a fixed learning rate to work with ReduceLROnPlateau\n",
        "learning_rate = 2e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "maxVit_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "nSRLqDLL3m6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = get_callbacks(model_name='maxvit_model')\n",
        "\n",
        "maxVit_history = maxVit_model.fit(\n",
        "    train_gen_new,\n",
        "    validation_data=valid_gen_new,\n",
        "    epochs=80,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "G-0u1uGA3myQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(maxVit_history)"
      ],
      "metadata": {
        "id": "2NxX9Rka3mut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = test_gen_new.classes\n",
        "predictions = maxVit_model.predict(test_gen_new)\n",
        "predicted_classes = np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "29gKZo073msa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_performance(test_labels, predicted_classes, list(test_gen_new.class_indices.keys()), figsize=(10, 8), cmap='Blues')"
      ],
      "metadata": {
        "id": "v_-9gzqp3mSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_model_results(\"MaxViT\", maxVit_history, test_gen_new, maxVit_model)\n",
        "display_results_table()"
      ],
      "metadata": {
        "id": "O9kKnqE9HS7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UMMN0MBuIeUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Trained Models"
      ],
      "metadata": {
        "id": "2EXsEpzf2nou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Y4HcIu_o2euA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n"
      ],
      "metadata": {
        "id": "wngrSTqN2eql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ygBPE16i04hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "xyBVnMTiFZhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, df, feature_extractor):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.feature_extractor = feature_extractor\n",
        "        # self.size = self.feature_extractor.size[\"shortest_edge\"] if \"shortest_edge\" in self.feature_extractor.size else 224\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.df.loc[idx, 'image_path']\n",
        "        label = int(self.df.loc[idx, 'label_encoded'])\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        # Apply feature extractor\n",
        "        encoding = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            'pixel_values': encoding['pixel_values'].squeeze(),\n",
        "            'label': torch.tensor(label)\n",
        "        }"
      ],
      "metadata": {
        "id": "SD-FTEx12emc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curves(history_dict):\n",
        "    \"\"\"\n",
        "    Plots training and validation accuracy & loss side by side.\n",
        "\n",
        "    Parameters:\n",
        "    - history_dict: dict returned by train_model\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Accuracy\n",
        "    axes[0].plot(history_dict[\"train_acc\"], label=\"Train\")\n",
        "    axes[0].plot(history_dict[\"val_acc\"], label=\"Validation\")\n",
        "    axes[0].set_title(\"Model Accuracy\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Accuracy\")\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Loss\n",
        "    axes[1].plot(history_dict[\"train_loss\"], label=\"Train\")\n",
        "    axes[1].plot(history_dict[\"val_loss\"], label=\"Validation\")\n",
        "    axes[1].set_title(\"Model Loss\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"Loss\")\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "WHpjJLY-FcMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
        "            inputs = batch['pixel_values'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(inputs).logits\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "NqMUuJOMG7bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diagnostic Code Block"
      ],
      "metadata": {
        "id": "jjdVxoQvhtHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check Class Balance\n",
        "def check_class_distribution(y, title='Class Distribution'):\n",
        "    counter = Counter(y)\n",
        "    classes = list(counter.keys())\n",
        "    counts = list(counter.values())\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(x=classes, y=counts)\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    print(\"Sample count per class:\", dict(counter))\n",
        "\n",
        "# 2. Check Train-Val Split Leakage\n",
        "def check_data_leak(train_paths, val_paths):\n",
        "    train_set = set(train_paths)\n",
        "    val_set = set(val_paths)\n",
        "    common = train_set.intersection(val_set)\n",
        "    print(f\"🔍 Common images between train and validation sets: {len(common)}\")\n",
        "    if len(common) > 0:\n",
        "        print(\"⚠️ Potential data leakage! Review your split logic.\")\n",
        "    else:\n",
        "        print(\"✅ No image overlap between train and validation sets.\")\n",
        "\n",
        "# 3. Confusion Matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PmfHPiZ5hsdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_paths = train_df_new['image_path'].tolist()\n",
        "val_image_paths = valid_df_new['image_path'].tolist()\n",
        "\n",
        "y_train = train_df_new['label_encoded'].tolist()\n",
        "y_val = valid_df_new['label_encoded'].tolist()"
      ],
      "metadata": {
        "id": "z1qBqTn0iJnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_class_distribution(y_train, \"Train Class Distribution\")\n",
        "check_class_distribution(y_val, \"Validation Class Distribution\")\n",
        "check_data_leak(train_image_paths, val_image_paths)"
      ],
      "metadata": {
        "id": "kV2YJTYiiJXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pretrained_evaluation(model, test_loader, device, class_names, model_name):\n",
        "    \"\"\"\n",
        "    Runs inference on the test_loader with progress bar and uses evaluate_model_performance.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
        "            inputs = batch['pixel_values'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(inputs).logits\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n===== {model_name} Evaluation =====\")\n",
        "    evaluate_model_performance(y_true, y_pred, class_names)\n"
      ],
      "metadata": {
        "id": "g5QXEQknud-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def record_model_results_hf(model_name, train_acc, val_acc, test_loader, model, device):\n",
        "    \"\"\"\n",
        "    Records results for Hugging Face / PyTorch models.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): Model identifier\n",
        "    - train_acc (list): Training accuracies per epoch\n",
        "    - val_acc (list): Validation accuracies per epoch\n",
        "    - test_loader: PyTorch DataLoader for test data\n",
        "    - model: Trained Hugging Face model\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \"\"\"\n",
        "    y_true, y_pred = [], []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs = batch['pixel_values'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(inputs).logits\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "    # Compute test accuracy\n",
        "    test_acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
        "\n",
        "    model_results.append({\n",
        "        'Model': model_name,\n",
        "        'Train Acc': round(train_acc[-1] * 100, 2),\n",
        "        'Val Acc': round(val_acc[-1] * 100, 2),\n",
        "        'Test Acc': round(test_acc * 100, 2),\n",
        "        'Precision': round(precision * 100, 2),\n",
        "        'Recall': round(recall * 100, 2),\n",
        "        'F1 Score': round(f1 * 100, 2)\n",
        "    })\n"
      ],
      "metadata": {
        "id": "o31wRvuns38e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT Pretained"
      ],
      "metadata": {
        "id": "Dcg285j_9jBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
      ],
      "metadata": {
        "id": "6Mv67DOS8rp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model_pretrained = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=len(label_encoder.classes_)\n",
        ")\n",
        "vit_model_pretrained.to('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "vpo95MKa2eib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(vit_model_pretrained.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "f9kjCKR52egw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    train_acc, val_acc = [], []\n",
        "    train_losses, val_losses = [], []\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        correct, total, train_loss = 0, 0, 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\"):\n",
        "            inputs = batch['pixel_values'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(inputs).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_train_acc = correct / total\n",
        "        epoch_train_loss = train_loss / len(train_loader)\n",
        "        train_acc.append(epoch_train_acc)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct, total, val_loss = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "                inputs = batch['pixel_values'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(inputs).logits\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        epoch_val_acc = correct / total\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        val_acc.append(epoch_val_acc)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: \"\n",
        "              f\"Train Acc = {epoch_train_acc:.4f}, Val Acc = {epoch_val_acc:.4f}, \"\n",
        "              f\"Train Loss = {epoch_train_loss:.4f}, Val Loss = {epoch_val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if epoch_val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_acc\n",
        "            torch.save(model.state_dict(), \"best_vit_hf_model.pth\")\n",
        "\n",
        "    return {\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_acc\": val_acc,\n",
        "        \"train_loss\": train_losses,\n",
        "        \"val_loss\": val_losses\n",
        "    }"
      ],
      "metadata": {
        "id": "-cs8uL022eer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_hf = CustomImageDataset(train_df_new, feature_extractor)\n",
        "val_dataset_hf = CustomImageDataset(valid_df_new, feature_extractor)\n",
        "test_dataset_hf = CustomImageDataset(test_df_new, feature_extractor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset_hf, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset_hf, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset_hf, batch_size=16)"
      ],
      "metadata": {
        "id": "ONgMO4-j8Tix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist_vit = train_model(vit_model_pretrained, train_loader, val_loader, epochs=6)"
      ],
      "metadata": {
        "id": "OnsOsKJr2eck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(hist_vit)"
      ],
      "metadata": {
        "id": "GepoCjJYF2K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate_model(vit_model_pretrained, test_loader)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "run_pretrained_evaluation(vit_model_pretrained, test_loader, device, class_names, \"ViT (Pretrained)\")"
      ],
      "metadata": {
        "id": "35JOGnNT2eai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_model_results_hf(\n",
        "    \"ViT (Pretrained)\",\n",
        "    hist_vit[\"train_acc\"],\n",
        "    hist_vit[\"val_acc\"],\n",
        "    test_loader,\n",
        "    vit_model_pretrained,\n",
        "    device\n",
        ")\n",
        "display_results_table()"
      ],
      "metadata": {
        "id": "_xjsOFswqqWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Swin Pretrained"
      ],
      "metadata": {
        "id": "MvK8uflM9oH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForImageClassification"
      ],
      "metadata": {
        "id": "xAITJag62eYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Swin-Tiny preprocessor\n",
        "swin_model_name = \"microsoft/swin-tiny-patch4-window7-224\"\n",
        "swin_processor = AutoImageProcessor.from_pretrained(swin_model_name)\n",
        "\n",
        "# Load the pre-trained model state dictionary, excluding the classifier weights\n",
        "try:\n",
        "    state_dict = AutoModelForImageClassification.from_pretrained(swin_model_name).state_dict()\n",
        "    # Filter out the classifier weights and bias\n",
        "    state_dict = {k: v for k, v in state_dict.items() if 'classifier' not in k}\n",
        "\n",
        "    # Define a new Swin model with the correct number of labels\n",
        "    swin_model = AutoModelForImageClassification.from_pretrained(\n",
        "        swin_model_name,\n",
        "        num_labels=len(label_encoder.classes_),\n",
        "        ignore_mismatched_sizes=True # Keep this as a safeguard, though not strictly necessary with state dict loading\n",
        "    ).to(device)\n",
        "\n",
        "    # Load the filtered state dictionary into the new model\n",
        "    # The `strict=False` argument allows loading even if some keys (like the new classifier) are missing in the state_dict\n",
        "    swin_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    print(\"Pre-trained Swin model loaded successfully, excluding classifier weights.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading pre-trained Swin model: {e}\")\n",
        "\n",
        "\n",
        "# Define optimizer and loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(swin_model.parameters(), lr=1e-4)\n",
        "\n",
        "# Create datasets and data loaders\n",
        "swin_train_dataset_hf = CustomImageDataset(train_df_new, swin_processor)\n",
        "swin_val_dataset_hf = CustomImageDataset(valid_df_new, swin_processor)\n",
        "swin_test_dataset_hf = CustomImageDataset(test_df_new, swin_processor)\n",
        "\n",
        "swin_train_loader = DataLoader(swin_train_dataset_hf, batch_size=16, shuffle=True)\n",
        "swin_val_loader = DataLoader(swin_val_dataset_hf, batch_size=16)\n",
        "swin_test_loader = DataLoader(swin_test_dataset_hf, batch_size=16)"
      ],
      "metadata": {
        "id": "_gAZnH9T2eWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "hist_swin = train_model(swin_model, swin_train_loader, swin_val_loader, epochs=5)"
      ],
      "metadata": {
        "id": "8_JbLIyq2eUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(hist_swin)"
      ],
      "metadata": {
        "id": "kQUJyFtZF70o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate_model(swin_model, swin_test_loader)\n",
        "class_names = label_encoder.classes_\n",
        "run_pretrained_evaluation(swin_model, test_loader, device, class_names, \"Swin (Pretrained)\")"
      ],
      "metadata": {
        "id": "wjVQ_KL42eSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_model_results_hf(\n",
        "    \"Swin (Pretrained)\",\n",
        "    hist_swin[\"train_acc\"],\n",
        "    hist_swin[\"val_acc\"],\n",
        "    test_loader,\n",
        "    swin_model,\n",
        "    device\n",
        ")\n",
        "display_results_table()"
      ],
      "metadata": {
        "id": "zdXO49cFtSSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MaxVit Pretrained"
      ],
      "metadata": {
        "id": "VHmR2VJsbE-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, AutoConfig"
      ],
      "metadata": {
        "id": "diJNtMqS2eP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxvit_model_name = \"timm/maxvit_tiny_rw_224.sw_in1k\"\n",
        "\n",
        "maxvit_processor = AutoImageProcessor.from_pretrained(\n",
        "    maxvit_model_name,\n",
        "    do_resize=True,\n",
        "    size={\"shortest_edge\": 224},\n",
        "    do_normalize=True\n",
        ")\n",
        "\n",
        "maxvit_model = AutoModelForImageClassification.from_pretrained(\n",
        "    maxvit_model_name,\n",
        "    num_labels=len(label_encoder.classes_),\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "lJJ-5vgV2eNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(maxvit_model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "VO2dVaOq4-qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxVit_train_dataset_hf = CustomImageDataset(train_df_new, maxvit_processor)\n",
        "maxVit_val_dataset_hf = CustomImageDataset(valid_df_new, maxvit_processor)\n",
        "maxVit_test_dataset_hf = CustomImageDataset(test_df_new, maxvit_processor)\n",
        "\n",
        "maxVit_train_loader = DataLoader(maxVit_train_dataset_hf, batch_size=4, shuffle=True)\n",
        "maxVit_val_loader = DataLoader(maxVit_val_dataset_hf, batch_size=4)\n",
        "maxVit_test_loader = DataLoader(maxVit_test_dataset_hf, batch_size=4)"
      ],
      "metadata": {
        "id": "o59ZY5_H2eEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7NkG5lDd7UpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "hist_maxvit = train_model(maxvit_model, maxVit_train_loader, maxVit_val_loader, epochs=5)"
      ],
      "metadata": {
        "id": "5iFfr94z2eCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(hist_maxvit)"
      ],
      "metadata": {
        "id": "wo892GeIGABh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate_model(maxvit_model, maxVit_test_loader)\n",
        "class_names = label_encoder.classes_\n",
        "run_pretrained_evaluation(maxvit_model, test_loader, device, class_names, \"MaxViT (Pretrained)\")"
      ],
      "metadata": {
        "id": "fxiWq-uC2dsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_model_results_hf(\n",
        "    \"MaxVit (Pretrained)\",\n",
        "    hist_maxvit[\"train_acc\"],\n",
        "    hist_maxvit[\"val_acc\"],\n",
        "    test_loader,\n",
        "    maxvit_model,\n",
        "    device\n",
        ")\n",
        "display_results_table()"
      ],
      "metadata": {
        "id": "oWXSCKvTtdJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PS1aMtx5wnee"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}